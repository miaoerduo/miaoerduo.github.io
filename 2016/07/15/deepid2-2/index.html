<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    基于Caffe的DeepID2实现（中） |
    
    喵耳朵</title>
  
  <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

  <link href="https://cdn.bootcdn.net/ajax/libs/firacode/5.2.0/fira_code.min.css" rel="stylesheet">
  <link href="https://cdn.bootcdn.net/ajax/libs/prism/1.24.1/themes/prism-okaidia.min.css" rel="stylesheet">
  <link href="https://cdn.bootcdn.net/ajax/libs/prism/1.24.1/plugins/line-numbers/prism-line-numbers.css" rel="stylesheet">
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="喵耳朵" type="application/atom+xml">
</head>

<body>
  <main class="content">
    <section class="outer">
  <article id="post-deepid2-2" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
      

<h1 class="article-title" itemprop="name">
  基于Caffe的DeepID2实现（中）
</h1>



    </header>
    

    
    <div class="article-meta">
      <a href="/2016/07/15/deepid2-2/" class="article-date">
  <time datetime="2016-07-15T15:06:34.000Z" itemprop="datePublished">2016-07-15</time>
</a>
      
<div class="article-category">
  <a class="article-category-link" href="/categories/Deep-Learning/">Deep Learning</a>
</div>

      <div class="wordcount wordcount-num">1.9k 字</div>
      <div class="wordcount wordcount-time">大约需要 8 分钟</div>
    </div>
    

    
    
<div class="tocbot"></div>

    

    <div class="article-entry" itemprop="articleBody">
      
      
      
      <blockquote>
<p>小喵的唠叨话：我们在上一篇博客里面，介绍了Caffe的Data层的编写。有了Data层，下一步则是如何去使用生成好的训练数据。也就是这一篇的内容。  </p>
</blockquote>
<span id="more"></span>
<h2 id="二精髓deepid2-loss层">二、精髓，DeepID2 Loss层</h2>
<p>DeepID2这篇论文关于verification signal的部分，给出了一个用于监督verification的loss。</p>
<p><img src="verification_loss.jpg" alt="verification_loss" /></p>
<p>其中，<span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(f_j\)</span> 是归一化之后的特征。</p>
<p>当 <span class="math inline">\(f_i\)</span> 和<span class="math inline">\(f_j\)</span> 属于同一个identity的时候，也就是 <span class="math inline">\(y_{ij}=1\)</span> 时，loss是二者的L2距离，约束使得特征更为相近。</p>
<p>当 <span class="math inline">\(f_i\)</span> 和 <span class="math inline">\(f_j\)</span> 不属于同一个identity的时候，即 <span class="math inline">\(y_{ij}=-1\)</span> ，这时的loss表示什么呢？参数m又表示什么？</p>
<p>m在这里是margin的意思，是一个可以自行设置的参数，表示期望的不同identity的feature之间的距离。当两个feature的大于margin时，说明网络已经可以很好的区分这两个特征，因此这是loss为0，当feature间的距离小于margin时，loss则为<span class="math inline">\((m-|f_i - f_j|)^2\)</span>，表示还需要两个特征能够更好的区分。因此这个loss函数比较好的反应了我们的需求，也就是DeepID2的算法思想。</p>
<p>这个Loss层实现起来似乎并不麻烦，前馈十分的简单。至于后馈，求导也非常简单。但是Caffe加入新层，需要在caffe.proto文件中，做一些修改，这也是最困扰小喵的地方。</p>
<p>不过有个好消息就是：Caffe官网增加了ContrastiveLossLayer这个层！和我们的需要是一样的。因此我们不需要自己实现这个层。</p>
<p>喜大普奔之余，小喵也专门看了Caffe的文档，以及这里提到了siamese network，发现这个网络使用ContrastiveLossLayer的方式比较独特，Caffe项目中的examples中有例子，感兴趣可以看看。</p>
<p>ContrastiveLossLayer的输入，也就是bottom有三部分，feature1、feature2、label，feature1和feature2是分别对应的两组feature，而label则表示该对feature是否是属于同一个identity，是的话，则为1，不是则为0。而且该层还提供一个参数margin，也就是论文的公式里面的m。</p>
<p>最终的结论就是，虽然我们不需要自己写Loss层，但是还是必须增加一些额外的层。</p>
<p>主要有2个，用于将特征归一化的NormalizationLayer以及用于将feature层转换成ContrastiveLossLayer的输入的层，不妨命名为ID2SliceLayer。</p>
<h2 id="三小问题大智慧之normalization-layer">三、小问题，大智慧之Normalization Layer</h2>
<p>这个归一化的层用于将输入的feature map进行归一化。Caffe官网并没有提供相关的层，因此我们必须自己实现（或者从网上找），这里我们还是选择自己来实现，顺便学习一下Caffe加层的技巧。</p>
<p>Normalization层的前馈非常的简单，输入为一个向量x，输出为归一化之后的向量：</p>
<p><span class="math display">\[f(\vec x)=\frac{\vec x}{\left\| \vec x \right \|}\]</span></p>
<p>至于后馈，需要求导，计算稍微有点复杂，小喵在推导4遍之后才给出如下表达式：</p>
<p><span class="math display">\[\frac{\partial \vec f}{\partial \vec x}=-\frac{1}{\left\| \vec x \right\|}*{\vec x}*{\vec x^T}+\frac{1}{\left\| \vec x \right\|}\]</span></p>
<p>其中x为输入的特征向量，为列向量。这里是将整个feature map看做一个列向量。</p>
<p>知道了前馈后馈的计算规则，那么很容易编写自己的层了，这里小喵建议大家找个Caffe已经有了的内容相近的层，照着改写。比如这个Normalization层，没有任何层的参数，所以照着ReLU类似的层就很好编写。</p>
<p>之后就祭出我们的code：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// create by miao</span>
<span class="token comment">// 主要实现了feature的归一化</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">ifndef</span> <span class="token expression">CAFFE_NORMALIZATION_LAYER_HPP_</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">CAFFE_NORMALIZATION_LAYER_HPP_</span></span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/blob.hpp"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/layer.hpp"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/proto/caffe.pb.h"</span></span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/layers/neuron_layer.hpp"</span></span>

<span class="token keyword">namespace</span> caffe <span class="token punctuation">&#123;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span>
<span class="token keyword">class</span> <span class="token class-name">NormalizationLayer</span> <span class="token operator">:</span> <span class="token base-clause"><span class="token keyword">public</span> <span class="token class-name">NeuronLayer</span><span class="token operator">&lt;</span><span class="token class-name">Dtype</span><span class="token operator">></span></span> <span class="token punctuation">&#123;</span>
 <span class="token keyword">public</span><span class="token operator">:</span>
  <span class="token keyword">explicit</span> <span class="token function">NormalizationLayer</span><span class="token punctuation">(</span><span class="token keyword">const</span> LayerParameter<span class="token operator">&amp;</span> param<span class="token punctuation">)</span>
      <span class="token operator">:</span> <span class="token generic-function"><span class="token function">NeuronLayer</span><span class="token generic class-name"><span class="token operator">&lt;</span>Dtype<span class="token operator">></span></span></span><span class="token punctuation">(</span>param<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span><span class="token punctuation">&#125;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">void</span> <span class="token function">LayerSetUp</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span>
      <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">inline</span> <span class="token keyword">const</span> <span class="token keyword">char</span><span class="token operator">*</span> <span class="token function">type</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token string">"Normalization"</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">inline</span> <span class="token keyword">int</span> <span class="token function">ExactNumBottomBlobs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">inline</span> <span class="token keyword">int</span> <span class="token function">ExactNumTopBlobs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span>
  
 <span class="token keyword">protected</span><span class="token operator">:</span>
  <span class="token keyword">virtual</span> <span class="token keyword">void</span> <span class="token function">Forward_cpu</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span>
      <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">void</span> <span class="token function">Forward_gpu</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span>
      <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">void</span> <span class="token function">Backward_cpu</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">,</span>
      <span class="token keyword">const</span> vector<span class="token operator">&lt;</span><span class="token keyword">bool</span><span class="token operator">></span><span class="token operator">&amp;</span> propagate_down<span class="token punctuation">,</span> <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">virtual</span> <span class="token keyword">void</span> <span class="token function">Backward_gpu</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">,</span>
      <span class="token keyword">const</span> vector<span class="token operator">&lt;</span><span class="token keyword">bool</span><span class="token operator">></span><span class="token operator">&amp;</span> propagate_down<span class="token punctuation">,</span> <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">)</span><span class="token punctuation">;</span>
  Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span> norm_val_<span class="token punctuation">;</span> <span class="token comment">// 记录每个feature的模</span>
<span class="token punctuation">&#125;</span><span class="token punctuation">;</span>

<span class="token punctuation">&#125;</span>  <span class="token comment">// namespace caffe</span>

<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span>  <span class="token comment">// CAFFE_NORMALIZATION_LAYER_HPP_</span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这个层的头文件异常的简单，和ReLU的仅有的区别就是类的名字不一样，而且多了个成员变量norm_val_，用来记录每个feature的模值。</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// create by miao</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cmath></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/layers/normalization_layer.hpp"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/util/math_functions.hpp"</span></span>

<span class="token keyword">namespace</span> caffe <span class="token punctuation">&#123;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token class-name">NormalizationLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">LayerSetUp</span><span class="token punctuation">(</span><span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span>
        <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token class-name">NeuronLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">LayerSetUp</span><span class="token punctuation">(</span>bottom<span class="token punctuation">,</span> top<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token function">CHECK_NE</span><span class="token punctuation">(</span>top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token keyword">this</span><span class="token operator">-></span><span class="token function">type</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">" Layer does not "</span>
        <span class="token string">"allow in-place computation."</span><span class="token punctuation">;</span>
    norm_val_<span class="token punctuation">.</span><span class="token function">Reshape</span><span class="token punctuation">(</span>bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">shape</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 申请norm的内存</span>
<span class="token punctuation">&#125;</span>


<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span> 
<span class="token keyword">void</span> <span class="token class-name">NormalizationLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">Forward_cpu</span><span class="token punctuation">(</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span> <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>

    Dtype <span class="token operator">*</span>norm_val_cpu_data <span class="token operator">=</span> norm_val_<span class="token punctuation">.</span><span class="token function">mutable_cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> n <span class="token operator">&lt;</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">shape</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token operator">++</span> n<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        <span class="token comment">// 计算每个c * h * w的区域的模</span>
        norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">=</span> std<span class="token double-colon punctuation">::</span><span class="token function">sqrt</span><span class="token punctuation">(</span><span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">></span></span></span><span class="token punctuation">(</span>
                    <span class="token generic-function"><span class="token function">caffe_cpu_dot</span><span class="token generic class-name"><span class="token operator">&lt;</span>Dtype<span class="token operator">></span></span></span><span class="token punctuation">(</span>
                        bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                        bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                        bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span>
                        <span class="token punctuation">)</span>
                    <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
        <span class="token comment">// 将每个bottom归一化，输出到top</span>
        <span class="token generic-function"><span class="token function">caffe_cpu_scale</span><span class="token generic class-name"><span class="token operator">&lt;</span>Dtype<span class="token operator">></span></span></span><span class="token punctuation">(</span>
                top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                <span class="token number">1.</span> <span class="token operator">/</span> norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">,</span> 
                bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">,</span> 
                top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">mutable_cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token class-name">NormalizationLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">Backward_cpu</span><span class="token punctuation">(</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">,</span> 
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span><span class="token keyword">bool</span><span class="token operator">></span><span class="token operator">&amp;</span> propagate_down<span class="token punctuation">,</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    
    <span class="token keyword">const</span> Dtype <span class="token operator">*</span>norm_val_cpu_data <span class="token operator">=</span> norm_val_<span class="token punctuation">.</span><span class="token function">cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">const</span> Dtype <span class="token operator">*</span>top_diff <span class="token operator">=</span> top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">cpu_diff</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    Dtype <span class="token operator">*</span>bottom_diff <span class="token operator">=</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">mutable_cpu_diff</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">const</span> Dtype <span class="token operator">*</span>bottom_data <span class="token operator">=</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">cpu_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

    <span class="token function">caffe_copy</span><span class="token punctuation">(</span>top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">count</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> top_diff<span class="token punctuation">,</span> bottom_diff<span class="token punctuation">)</span><span class="token punctuation">;</span>
    
    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> n <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> n <span class="token operator">&lt;</span> top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">shape</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token operator">++</span> n<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
        Dtype a <span class="token operator">=</span> <span class="token operator">-</span> <span class="token number">1.</span><span class="token operator">/</span><span class="token punctuation">(</span>norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token operator">*</span> norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token generic-function"><span class="token function">caffe_cpu_dot</span><span class="token generic class-name"><span class="token operator">&lt;</span>Dtype<span class="token operator">></span></span></span><span class="token punctuation">(</span>
                top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                top_diff <span class="token operator">+</span> top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">,</span>
                bottom_data <span class="token operator">+</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">;</span>
        Dtype b <span class="token operator">=</span> <span class="token number">1.</span> <span class="token operator">/</span> norm_val_cpu_data<span class="token punctuation">[</span>n<span class="token punctuation">]</span><span class="token punctuation">;</span>
        <span class="token generic-function"><span class="token function">caffe_cpu_axpby</span><span class="token generic class-name"><span class="token operator">&lt;</span>Dtype<span class="token operator">></span></span></span><span class="token punctuation">(</span>
                top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">count</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                a<span class="token punctuation">,</span>
                bottom_data <span class="token operator">+</span> bottom<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">,</span>
                b<span class="token punctuation">,</span>
                bottom_diff <span class="token operator">+</span> top<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">-></span><span class="token function">offset</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span>
                <span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">&#125;</span>
<span class="token punctuation">&#125;</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">ifdef</span> <span class="token expression">CPU_ONLY</span></span>
<span class="token function">STUB_GPU</span><span class="token punctuation">(</span>NormalizationLayer<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span></span>

<span class="token function">INSTANTIATE_CLASS</span><span class="token punctuation">(</span>NormalizationLayer<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">REGISTER_LAYER_CLASS</span><span class="token punctuation">(</span>Normalization<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token punctuation">&#125;</span> <span class="token comment">// namespace caffe</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最后就是GPU部分的代码，如果不在乎性能的话，直接在CUDA的前后馈里面调用CPU版的前后馈就行。当然如果了解CUDA的话，完全可以写一份GPU版的代码。小喵这里就偷懒了一下。。。</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token comment">// create by miao</span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;vector></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;cmath></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/layers/normalization_layer.hpp"</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">"caffe/util/math_functions.hpp"</span></span>

<span class="token keyword">namespace</span> caffe <span class="token punctuation">&#123;</span>

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span> 
<span class="token keyword">void</span> <span class="token class-name">NormalizationLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">Forward_gpu</span><span class="token punctuation">(</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">,</span> <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">this</span><span class="token operator">-></span><span class="token function">Forward_cpu</span><span class="token punctuation">(</span>bottom<span class="token punctuation">,</span> top<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>   

<span class="token keyword">template</span> <span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">Dtype</span><span class="token operator">></span>
<span class="token keyword">void</span> <span class="token class-name">NormalizationLayer</span><span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token double-colon punctuation">::</span><span class="token function">Backward_gpu</span><span class="token punctuation">(</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> top<span class="token punctuation">,</span> 
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span><span class="token keyword">bool</span><span class="token operator">></span><span class="token operator">&amp;</span> propagate_down<span class="token punctuation">,</span>
    <span class="token keyword">const</span> vector<span class="token operator">&lt;</span>Blob<span class="token operator">&lt;</span>Dtype<span class="token operator">></span><span class="token operator">*</span><span class="token operator">></span><span class="token operator">&amp;</span> bottom<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>
    <span class="token keyword">this</span><span class="token operator">-></span><span class="token function">Backward_cpu</span><span class="token punctuation">(</span>top<span class="token punctuation">,</span> propagate_down<span class="token punctuation">,</span> bottom<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span>
<span class="token function">INSTANTIATE_LAYER_GPU_FUNCS</span><span class="token punctuation">(</span>NormalizationLayer<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">&#125;</span> <span class="token comment">// namespace caffe</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>这样，我们就写完了Normalization层的所有代码。</p>
<p>对于比较老版本的Caffe，还需要修改/caffe_root/src/caffe/caffe.proto文件。而新版的Caffe只要在新增参数的情况下才需要修改。我们的这个Normalization层并没有用到新的参数，因此并不需要修改caffe.proto文件。</p>
<p>至于新版的Caffe为什么这么智能，原因其实就在这两行代码：</p>
<pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token function">INSTANTIATE_CLASS</span><span class="token punctuation">(</span>NormalizationLayer<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token function">REGISTER_LAYER_CLASS</span><span class="token punctuation">(</span>Normalization<span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>宏INSTANTIATE_CLASS在/caffe_root/include/caffe/common.hpp中定义。</p>
<p>宏REGISTER_LAYER_CLASS在/caffe_root/include/caffe/layer_factory.hpp中定义。</p>
<p>感兴趣可以自行查阅。</p>
<div style="background-color: #ddd; padding: 20px">
<p>重要更新:</br> 小喵最近训练的时候实际上已经不使用Normalization层了，而是将Contrastive Loss直接接在feature层的后面，同时由于训练数据都是正样本对，那么margin就没有意义了。不过比较麻烦的是loss weight的选取。</p>
</div>
<p><strong>转载</strong>请注明出处~</p>

      
    </div>
    <footer class="article-footer">
      
        <div>
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>
        作者: 
      </strong>
      Zhao Yu</a>
    </li>
    <li class="post-copyright-link">
      <strong>
        文章链接: 
      </strong>
      <a href="/2016/07/15/deepid2-2/" target="_blank" title="基于Caffe的DeepID2实现（中）">
        https://www.miaoerduo.com/2016/07/15/deepid2-2/
      </a>
    </li>
    <li class="post-copyright-license">
      <strong>
        版权声明: 
      </strong>
      本网站所有文章除特别声明外,均采用 <a rel="license"
          href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh-Hans" target="_blank"
          title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-SA 4.0)">CC BY-SA 4.0</a>
        许可协议。转载请注明出处!
    </li>
  </ul>
<div>
      
      
      
    </footer>

  </div>

  
  
<nav class="article-nav">
  
  <a href="/2016/07/15/deepid2-3/" class="article-nav-link">
    <strong class="article-nav-caption">前一篇</strong>
    <div class="article-nav-title">
      
      基于Caffe的DeepID2实现（下）
      
    </div>
  </a>
  
  
  <a href="/2016/07/13/deepid2-1/" class="article-nav-link">
    <strong class="article-nav-caption">后一篇</strong>
    <div class="article-nav-title">基于Caffe的DeepID2实现（上）</div>
  </a>
  
</nav>

  

  
  
<div class="vcomments" id="vcomments"></div>

<script src="https://unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  new Valine({
    el: '#vcomments',
    appId: 'mABH5OG3C2B5Ix9WzfF7vjiE-gzGzoHsz',
    appKey: 'EObjAwnwQdyltsmlS48XLJ19',
    notify: 'true',
    verify: 'true',
    avatar: 'mp',
    pageSize: '10',
    placeholder: '请输入...'
  })
</script>

  
  

</article>
  
    
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
    });
    MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
    });
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  
</section>
    <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>喵耳朵 &copy; 2025</li>
      
        <li><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" class="beian">京ICP备16004318号-1</a></li>
      
    </ul>
  </div>
</footer>
  </main>
  <aside class="sidebar">
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/MED-logo-black.png" alt="喵耳朵"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">首页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Architecture">架构</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Illustration">插画</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="fe fe-feed"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>






<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
  });
</script>



<script src="/js/ocean.js"></script>

</body>

</html>